{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36f5e93-e39d-48b5-bca3-c9aa64937e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676a87f1-381d-492b-a512-f9f25d5f8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeAnalyser:\n",
    "    def __init__(self, resume) -> None:\n",
    "        \"\"\"\n",
    "        Info:\n",
    "          A Resume vector store function\n",
    "        Args:\n",
    "          resume[Document] --> A pdf document\n",
    "        Returns:\n",
    "          None\n",
    "        \"\"\"\n",
    "        self.resume = resume\n",
    "        self.pdf_loder = PyMuPDFLoader\n",
    "        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.human_template = \"\"\"{question}\"\"\"\n",
    "        self.system_template = \"You are AI assiatant that can analyse the given text and answer the questions according based on given text be more specific about answer: {text}\"\n",
    "        self.collection_name = \"resume_collections\"\n",
    "        self.client_settings = {\n",
    "            \"host\": \"192.168.43.163\",\n",
    "            \"port\": \"19530\"}\n",
    "        self.config = {\n",
    "            'max_new_tokens': 512,\n",
    "            'repetition_penalty': 1.1,\n",
    "            'context_length': 2000,\n",
    "        }\n",
    "        self.model = CTransformers(\n",
    "            model='C:\\\\Users\\\\Saiprasad\\\\.cache\\\\huggingface\\\\hub\\\\models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF\\\\snapshots\\\\3a6fbf4a41a1d52e415a4958cde6856d34b2db93\\\\mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
    "            model_type='llama', lib='avx', config=self.config, callback_manager=self.callback_manager)\n",
    "    def Analyse(self, param1, param2):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "          This function used cosine simalarity to analyse the resume\n",
    "        Args:\n",
    "          resume --> Input Resume\n",
    "        Returns:\n",
    "          str --> Returns data based on Cosine-similarity\n",
    "        \"\"\"\n",
    "        resume_data = self.pdf_loder(self.resume)\n",
    "        resumes_data = resume_data.load()\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\",\n",
    "            chunk_size=2500,\n",
    "            chunk_overlap=10,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False)\n",
    "        data = text_splitter.split_documents(resumes_data)\n",
    "        db = Milvus.from_documents(\n",
    "            data, \n",
    "            self.embedding_model, \n",
    "            connection_args=self.client_settings, \n",
    "            collection_name = self.collection_name)\n",
    "        docs = db.similarity_search(param1, k = 1)\n",
    "        human_prompt = HumanMessagePromptTemplate.from_template(self.human_template)\n",
    "        system_prompt = SystemMessagePromptTemplate.from_template(self.system_template)\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "        prompt = chat_prompt.format_prompt(question=param2, text=docs).to_messages()\n",
    "        return self.model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76263811-eb89-47fb-a8df-26e57853b5c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyser = ResumeAnalyser(\"../data/Saiprasad Toshatwad.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c01f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "AI: Based on the provided text from Saiprasad Toshatwad's resume, his skill sections include:\n",
      "\n",
      "1. SQL (MySQL, PostgreSQL), VectorDB’s\n",
      "2. Python, LangChain (Pandas, Tensorflow, Yellowbrick, Shap, NumPy, Scikit-learn, API, ORM.. etc.)\n",
      "3. Golang, ORM\n",
      "4. MLOps: Linux, Kedro, Docker, Git, DVC, MLflow, AWS, EC2, S3, botot3, AWS lambda, Github Actions\n",
      "5. PowerBI"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\".\\nAI: Based on the provided text from Saiprasad Toshatwad's resume, his skill sections include:\\n\\n1. SQL (MySQL, PostgreSQL), VectorDB’s\\n2. Python, LangChain (Pandas, Tensorflow, Yellowbrick, Shap, NumPy, Scikit-learn, API, ORM.. etc.)\\n3. Golang, ORM\\n4. MLOps: Linux, Kedro, Docker, Git, DVC, MLflow, AWS, EC2, S3, botot3, AWS lambda, Github Actions\\n5. PowerBI\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.Analyse(\"skills\", \"list down his skill sections on resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d0dac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as a Data Scientist?\n",
      "AI: Saiprasad Toshatwad worked as a Data Scientist at Eaton between 2023 and 2023."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' as a Data Scientist?\\nAI: Saiprasad Toshatwad worked as a Data Scientist at Eaton between 2023 and 2023.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.Analyse(\"Companies\", \"In which companies saiprasad toshatwad worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec3df32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "AI: Saiprasad mentions his projects that include the use of Golang, but there's no explicit statement about the number of years he has been a Golang developer. Therefore, I cannot provide an exact answer based on the given text."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"?\\nAI: Saiprasad mentions his projects that include the use of Golang, but there's no explicit statement about the number of years he has been a Golang developer. Therefore, I cannot provide an exact answer based on the given text.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.Analyse(\"Experiance\", \"How many years saiprasad have experiance as golang developer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcb0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Assistant: Based on the provided text, Saiprasad has worked on several projects that involve data analysis and machine learning. However, the text doesn't explicitly mention the number of years he has spent as a data scientist. Therefore, I cannot provide an exact answer to this question based on the given text."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"?\\nAssistant: Based on the provided text, Saiprasad has worked on several projects that involve data analysis and machine learning. However, the text doesn't explicitly mention the number of years he has spent as a data scientist. Therefore, I cannot provide an exact answer to this question based on the given text.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.Analyse(\"Experiance\", \"How many years saiprasad have experiance as data scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e632006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Based on the provided text, Saiprasad has worked on several projects. However, there is no clear indication of the number of years he's had experience in total. The text mentions his engineering of a robust e-commerce backend application, development of a haze remover application, application of LSTM and deep learning, utilization of Kedro, Docker, PowerBI, and DAX techniques, but it does not provide the length of time he spent on these projects or his overall experience."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAssistant: Based on the provided text, Saiprasad has worked on several projects. However, there is no clear indication of the number of years he's had experience in total. The text mentions his engineering of a robust e-commerce backend application, development of a haze remover application, application of LSTM and deep learning, utilization of Kedro, Docker, PowerBI, and DAX techniques, but it does not provide the length of time he spent on these projects or his overall experience.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.Analyse(\"Experiance\", \"How many years saiprasad have experiance in total?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7295151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and work experience.\n",
      "Assistant: I'd be happy to help you with that! The text provides information about two personal projects and three work experiences for Saiprasad Toshatwad. Here they are:\n",
      "\n",
      "**Projects:**\n",
      "1. TextFlow: An advanced video-to-text summarization project using RabbitMQ, Docker, MongoDB, SQL, and a 3-tier architecture. He overcame challenges in text summarization and SaaS models to deliver the project.\n",
      "\n",
      "**Work Experience:**\n",
      "1. Awesomesuite: Golang Developer from April 2023 to present. At AwesomeSuite, he is responsible for spearheading the development of cutting-edge APIs for SaaS applications, utilizing AWS for deployment, and leading the charge in API creation, backend optimization, and deploying services on AWS Lambda.\n",
      "2. Eaton: Data Scientist from August 2023 to November 2023. During his internship at Eaton, he was a core team member in the Generative Design project, where he led various image-focused projects including Mask RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1201) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1202) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1203) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1204) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1205) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1206) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1207) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1208) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1209) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1210) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1211) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1212) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1213) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1214) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1215) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1216) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1217) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1218) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1219) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1220) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1221) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1222) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1223) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1224) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1225) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1226) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1227) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1228) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1229) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1230) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1231) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1232) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1233) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1234) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1235) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1236) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1237) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1238) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1239) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1240) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1241) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1242) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1243) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1244) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1245) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1246) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1247) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1248) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1249) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1250) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1251) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1252) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1253) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1254) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1255) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1256) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1257) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1258) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1259) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1260) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1261) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1262) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1263) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1264) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1265) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1266) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1267) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1268) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1269) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RC"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1270) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1271) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (1272) exceeded maximum context length (1200).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43manalyser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAnalyse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProjects\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist down his projects\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m, in \u001b[0;36mResumeAnalyser.Analyse\u001b[1;34m(self, param1, param2)\u001b[0m\n\u001b[0;32m     55\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([system_prompt, human_prompt])\n\u001b[0;32m     56\u001b[0m prompt \u001b[38;5;241m=\u001b[39m chat_prompt\u001b[38;5;241m.\u001b[39mformat_prompt(question\u001b[38;5;241m=\u001b[39mparam2, text\u001b[38;5;241m=\u001b[39mdocs)\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:230\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    227\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    228\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    231\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    232\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    233\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    234\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    235\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    236\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    238\u001b[0m         )\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:525\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    519\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    523\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    524\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:698\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    684\u001b[0m         )\n\u001b[0;32m    685\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    686\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    687\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     ]\n\u001b[1;32m--> 698\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    699\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    700\u001b[0m     )\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:562\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    561\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    563\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:549\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    541\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 549\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    550\u001b[0m                 prompts,\n\u001b[0;32m    551\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    552\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    553\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    554\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    555\u001b[0m             )\n\u001b[0;32m    556\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    557\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    558\u001b[0m         )\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1134\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1133\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1137\u001b[0m     )\n\u001b[0;32m   1138\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\langchain_community\\llms\\ctransformers.py:104\u001b[0m, in \u001b[0;36mCTransformers._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m text \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    103\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForLLMRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    105\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m    106\u001b[0m     _run_manager\u001b[38;5;241m.\u001b[39mon_llm_new_token(chunk, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\ctransformers\\llm.py:570\u001b[0m, in \u001b[0;36mLLM._stream\u001b[1;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[0;32m    568\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m incomplete \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    571\u001b[0m     tokens,\n\u001b[0;32m    572\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    573\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m    574\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    575\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[0;32m    576\u001b[0m     last_n_tokens\u001b[38;5;241m=\u001b[39mlast_n_tokens,\n\u001b[0;32m    577\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    578\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    579\u001b[0m     threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[0;32m    580\u001b[0m     reset\u001b[38;5;241m=\u001b[39mreset,\n\u001b[0;32m    581\u001b[0m ):\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;66;03m# Handle incomplete UTF-8 multi-byte characters.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m     incomplete \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize([token], decode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    584\u001b[0m     complete, incomplete \u001b[38;5;241m=\u001b[39m utf8_split_incomplete(incomplete)\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\ctransformers\\llm.py:537\u001b[0m, in \u001b[0;36mLLM.generate\u001b[1;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    530\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    531\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    535\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    536\u001b[0m     )\n\u001b[1;32m--> 537\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_eos_token(token):\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mE:\\LLM\\venv\\lib\\site-packages\\ctransformers\\llm.py:403\u001b[0m, in \u001b[0;36mLLM.eval\u001b[1;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[0;32m    399\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_past\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceeded maximum context length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m tokens \u001b[38;5;241m=\u001b[39m (c_int \u001b[38;5;241m*\u001b[39m n_tokens)(\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[1;32m--> 403\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctransformers_llm_batch_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m status:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to evaluate tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "analyser.Analyse(\"Projects\", \"list down his projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350f964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

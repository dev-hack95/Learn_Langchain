{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085a6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e98203",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd00691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfaebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\LLM\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "config = {'max_new_tokens': 256, 'repetition_penalty': 1.1}\n",
    "llm = CTransformers(model='c:/Users/Saiprasad/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-Chat-GGML/snapshots/76cd63c351ae389e1d4b91cab2cf470aab11864b/llama-2-7b-chat.ggmlv3.q4_K_M.bin',\n",
    "                    model_type='llama', lib='avx', config=config, callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8faaee6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\LLM\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in deep learning?\n",
      "In deep learning, bit quantization is a technique to reduce the precision of the weights and activations of neural networks by representing them using fewer bits. This can be useful for deploying models on devices with limited computing resources or for reducing the memory usage of the model. The goal of bit quantization is to maintain the accuracy of the model while reducing its computational requirements.\n",
      "\n",
      "There are different types of bit quantization techniques, including:\n",
      "\n",
      "1. Post-training quantization: In this approach, the full-precision model is first trained, and then the weights and activations are quantized to a lower precision.\n",
      "2. Quantization-aware training: In this approach, the model is trained from scratch with low-precision weights and activations.\n",
      "3. Weight binarization: In this approach, the weights are represented as +1 or -1, which can lead to significant reductions in memory usage and computational requirements.\n",
      "4. Trained ternary weights: In this approach, the weights are represented as fractions between 0 and 1, which can provide a good balance between accuracy and computational efficiency.\n",
      "\n",
      "Bit quantization can be applied to different layers of the network, including the"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' in deep learning?\\nIn deep learning, bit quantization is a technique to reduce the precision of the weights and activations of neural networks by representing them using fewer bits. This can be useful for deploying models on devices with limited computing resources or for reducing the memory usage of the model. The goal of bit quantization is to maintain the accuracy of the model while reducing its computational requirements.\\n\\nThere are different types of bit quantization techniques, including:\\n\\n1. Post-training quantization: In this approach, the full-precision model is first trained, and then the weights and activations are quantized to a lower precision.\\n2. Quantization-aware training: In this approach, the model is trained from scratch with low-precision weights and activations.\\n3. Weight binarization: In this approach, the weights are represented as +1 or -1, which can lead to significant reductions in memory usage and computational requirements.\\n4. Trained ternary weights: In this approach, the weights are represented as fractions between 0 and 1, which can provide a good balance between accuracy and computational efficiency.\\n\\nBit quantization can be applied to different layers of the network, including the'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"what is bit quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395445ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"\"\"\n",
    "Instructions:\n",
    "1) Modify the code to get minimum time complexicty\n",
    "\n",
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        output = set()\n",
    "        total = 0\n",
    "        n = len(nums)\n",
    "\n",
    "        if n < 3 and n > 3000:\n",
    "            raise Exception(\"Require at least three elements in the array\")\n",
    "\n",
    "        for num in nums:\n",
    "            if num < -10**5 or num > 10**5:\n",
    "                raise Exception(\"Element value should be between -10**5 and 10**5\")\n",
    "\n",
    "        nums = sorted(nums)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(n - 2):\n",
    "            for j in range(i + 1, n - 1):\n",
    "                for k in range(j + 1, n):\n",
    "                    if nums[i] + nums[j] + nums[k] == total:\n",
    "                        output.add((nums[i], nums[j], nums[k]))\n",
    "\n",
    "        return list(output)\n",
    "                        \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0b4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
